# -*- coding: utf-8 -*-
"""worked_Human_Activity_Detection_Upgraded_Model_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BiC1RNFykxTzejGY3hXJN1qZ7U9oOyhl

# Import Libraries
"""

from tensorflow.keras.models import load_model
import cv2
import numpy as np
import pandas as pd
from collections import deque
from moviepy.editor import *

# Download Model uncomment if running in Google Colab or Do not have the trained model
# !git clone https://github.com/kunaltulsidasani/Suspicious-Human-Activity-Detection-LRCN.git
# !mkdir Dataset
# !mv /content/Suspicious-Human-Activity-Detection-LRCN/main//content/Suspicious-Human-Activity-Detection-LRCN/Predict* /content/Dataset/

#!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vNp1c7ouaWtaHb5GdLZX6OV5ldpXwg_Z' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1vNp1c7ouaWtaHb5GdLZX6OV5ldpXwg_Z" -O Suspicious_Human_Activity_Detection_VGG16_LSTM_Model.h5 && rm -rf /tmp/cookies.txt

"""# Declare Constants"""

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 30

# Specify the directory containing the UCF50 dataset.
DATASET_DIR = "Dataset"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["nofights", "fights"]

"""# Load Model"""

model = load_model('Violance_Detection.h5')

"""# Prediction of Single Action"""

def predict_single_action(video_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform single action recognition prediction on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Declare a list to store video frames we will extract.
    frames_list = []

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Get the number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)

    # Iterating the number of times equal to the fixed length of sequence.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Read a frame.
        success, frame = video_reader.read()

        # Check if frame is not read properly then break the loop.
        if not success:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list
        frames_list.append(normalized_frame)

    # Passing the  pre-processed frames to the model and get the predicted probabilities.
    predicted_labels_probabilities = model.predict(np.expand_dims(frames_list, axis = 0))[0]

    # Get the index of class with highest probability.
    predicted_label = np.argmax(predicted_labels_probabilities)

    # Get the class name using the retrieved index.
    predicted_class_name = CLASSES_LIST[predicted_label]

    # Display the predicted action along with the prediction confidence.
    print(f'Action Predicted: {predicted_class_name}\nConfidence: {predicted_labels_probabilities[predicted_label]}')

    # Release the VideoCapture object.
    video_reader.release()

predict_single_action("/content/Suspicious-Human-Activity-Detection-LRCN/Predict/fight.avi",SEQUENCE_LENGTH)

predict_single_action("/content/Suspicious-Human-Activity-Detection-LRCN/Predict/running.avi",SEQUENCE_LENGTH)

predict_single_action("/content/Suspicious-Human-Activity-Detection-LRCN/Predict/walking.avi",SEQUENCE_LENGTH)

"""# All Action Prediction in Video"""

import cv2

def convert_fps(input_video_path, output_video_path, new_fps):
    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(input_video_path)

    # Get the original video's FPS and codec.
    original_fps = video_reader.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'DIVX')

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_video_path, fourcc, new_fps,
                                   (int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH)),
                                    int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))))

    # Process every n-th frame to achieve the desired FPS.
    frame_counter = 0
    while video_reader.isOpened():
        ok, frame = video_reader.read()

        if not ok:
            break

        if frame_counter % (original_fps // new_fps) == 0:
            video_writer.write(frame)

        frame_counter += 1

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'DIVX'),
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read()

        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]
            print(f'Action Predicted: {predicted_class_name}\nConfidence: {predicted_labels_probabilities[predicted_label]}')

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# predict_on_video("/content/Suspicious-Human-Activity-Detection-LRCN/Predict/Human-Activity.avi","Human-Activity-Prediction.avi",SEQUENCE_LENGTH)

VideoFileClip("Human-Activity-Prediction.avi", audio=False).ipython_display()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from moviepy.editor import VideoFileClip
# 
# def convert_mp4_to_avi(input_file, output_file):
#     clip = VideoFileClip(input_file)
#     clip.write_videofile(output_file, fps=25, codec='mpeg4')
# 
# input_file = 'Fight_sample_video.mp4'
# output_file = 'Fight_sample_video.avi'
# convert_mp4_to_avi(input_file, output_file)
# 
# new_fps = 5  # Desired frames per second
# 
# convert_fps(input_file, output_file, new_fps)
# predict_on_video("Fight_sample_video.avi","Fight_prediction.avi",SEQUENCE_LENGTH)

from moviepy.editor import VideoFileClip

def convert_video_to_50_seconds(input_file, output_file):
    clip = VideoFileClip(input_file)
    if clip.duration > 50:
        clip = clip.subclip(0, 50)
    clip.write_videofile(output_file, fps=25, codec='libx264')

input_file = 'Fight_sample_video.mp4'
output_file = 'smaller_fight_sample_video.mp4'
convert_video_to_50_seconds(input_file, output_file)
def convert_mp4_to_avi(input_file, output_file):
    clip = VideoFileClip(input_file)
    clip.write_videofile(output_file, fps=25, codec='mpeg4')

input_file = 'smaller_fight_sample_video.mp4'
output_file = 'smaller_fight_sample_video.avi'
convert_mp4_to_avi(input_file, output_file)

# new_fps = 5  # Desired frames per second

# convert_fps(input_file, output_file, new_fps)
predict_on_video("smaller_fight_sample_video.avi","smaller_Fight_prediction.avi",SEQUENCE_LENGTH)

VideoFileClip("smaller_Fight_prediction.avi", audio=False).ipython_display()

def convert_video_to_50_seconds(input_file, output_file):
    clip = VideoFileClip(input_file)
    if clip.duration > 50:
        clip = clip.subclip(0, 50)
    clip.write_videofile(output_file, fps=25, codec='libx264')

input_file = 'nofi010.mp4'

output_file = 'nofi011.avi'
convert_mp4_to_avi(input_file, output_file)

new_fps = 25  # Desired frames per second

convert_fps(input_file, output_file, new_fps)
predict_on_video('nofi011.avi',"nofi010_prediction.avi",SEQUENCE_LENGTH)

VideoFileClip("nofi010_prediction.avi", audio=False).ipython_display()

def doit(input_file):
  def convert_video_to_50_seconds(input_file, output_file):
    clip = VideoFileClip(input_file)
    if clip.duration > 50:
      clip = clip.subclip(0, 50)
    clip.write_videofile(output_file, fps=25, codec='libx264')


  output_file = 'nofi011.avi'
  convert_mp4_to_avi(input_file, output_file)

  # new_fps = 25  # Desired frames per second

  # convert_fps(input_file, output_file, new_fps)
  predict_single_action('nofi011.avi',SEQUENCE_LENGTH)
  predict_on_video('nofi011.avi',"nofi010_prediction.avi",SEQUENCE_LENGTH)

doit('fi052.mp4')

VideoFileClip("nofi010_prediction.avi", audio=False).ipython_display()

doit('fi110.mp4')

VideoFileClip("nofi010_prediction.avi", audio=False).ipython_display()

doit('nofi112.mp4')

doit('normal1.mp4')

VideoFileClip("nofi010_prediction.avi", audio=False).ipython_display()

IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64
SEQUENCE_LENGTH = 16
THRESHOLD_CONSECUTIVE_FRAMES = 10
CLASSES_LIST = ["NonViolence", "Violence"]

# def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):
#     video_reader = cv2.VideoCapture(video_file_path)
#     original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
#     original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))
#     video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),
#                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
#     frames_queue = deque(maxlen=SEQUENCE_LENGTH)
#     predicted_class_name = ''

#     while video_reader.isOpened():
#         ok, frame = video_reader.read()

#         if not ok:
#             break

#         resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
#         normalized_frame = resized_frame / 255
#         frames_queue.append(normalized_frame)

#         if len(frames_queue) == SEQUENCE_LENGTH:
#             predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis=0))[0]
#             predicted_label = np.argmax(predicted_labels_probabilities)
#             predicted_class_name = CLASSES_LIST[predicted_label]

#         if predicted_class_name == "Violence":
#             cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)
#         else:
#             cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)

#         video_writer.write(frame)

#     video_reader.release()
#     video_writer.release()

# plt.style.use("default")
# def show_pred_frames(pred_video_path):
#     plt.figure(figsize=(20, 15))
#     video_reader = cv2.VideoCapture(pred_video_path)
#     frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
#     random_range = sorted(random.sample(range(SEQUENCE_LENGTH, frames_count), 12))

#     for counter, random_index in enumerate(random_range, 1):
#         plt.subplot(5, 4, counter)
#         video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)
#         ok, frame = video_reader.read()

#         if not ok:
#             break

#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#         plt.imshow(frame)
#         ax.figure.set_size_inches(20, 20)
#         plt.tight_layout()

#     video_reader.release()
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

def detect_violence_video(model, video_file_path, sequence_length=SEQUENCE_LENGTH):
    # Open the video file
    cap = cv2.VideoCapture(video_file_path)

    # Variable to keep track of consecutive frames classified as violence
    violence_counter = 0
    frames_queue = []

    while cap.isOpened():
        # Capture frame-by-frame
        ret, frame = cap.read()

        if not ret:
            break

        # Resize the frame to match the input size of your model
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Preprocess the frame
        normalized_frame = resized_frame / 255

        # Add the frame to the frames queue
        frames_queue.append(normalized_frame)

        # If the frames queue has reached the sequence length, make a prediction
        if len(frames_queue) == sequence_length:
            # Predict the class label
            frames_array = np.array(frames_queue)
            frames_array = np.expand_dims(frames_array, axis=0)
            predicted_labels_probabilities = model.predict(frames_array)[0]
            predicted_label = np.argmax(predicted_labels_probabilities)
            predicted_class_name = CLASSES_LIST[predicted_label]

            # If violence is detected, increment the violence counter
            if predicted_class_name == "Violence":
                violence_counter += 1
                print("NOW violence")
            else:
                violence_counter = 0
                print("NOW NO violence")

            # If violence has been detected for a certain number of consecutive frames, take action
            if violence_counter >= THRESHOLD_CONSECUTIVE_FRAMES:
                # Perform the desired action (e.g., alerting authorities)
                print("Violence detected! Take action.")

                # Reset the violence counter
                violence_counter = 0

            # Remove the oldest frame from the queue
            frames_queue.pop(0)

        # Display the frame
        cv2_imshow(frame)

        # If 'q' is pressed, break from the loop
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Release the video file and close all OpenCV windows
    cap.release()
    cv2.destroyAllWindows()

# Usage example
video_file_path = "Fight_sample_video.mp4"  # Path to your MP4 video file
detect_violence_video(model, video_file_path)

import subprocess
def convert_avi_to_mp4(input_file, output_file):
    clip = VideoFileClip(input_file)
    clip.write_videofile(output_file, fps=25, codec='libx264')

avi_path = '/content/Suspicious-Human-Activity-Detection-LRCN/Predict/Human-Activity.avi'
mp4_path = 'output.mp4'
convert_avi_to_mp4(avi_path, mp4_path)
detect_violence_video(model, mp4_path)

from pytube import YouTube
def predict_video_from_url(video_url, sequence_length=16):
    try:
        # Get YouTube video details using pytube
        youtube_video = YouTube(video_url)
        video_stream = youtube_video.streams.filter(file_extension="mp4").first()
        video_stream_url = video_stream.url
        predict_frames(video_stream_url, output_video_file_path, sequence_length)
        show_pred_frames(output_video_file_path)


        # Perform prediction on the video
        predict_video(video_stream_url, sequence_length)

    except Exception as e:
        print(f"Error: {e}")

# Usage example
video_file_path = "normal2.mp4"  # Path to your MP4 video file
detect_violence_video(model, video_file_path)