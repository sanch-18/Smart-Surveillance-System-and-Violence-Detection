# -*- coding: utf-8 -*-
"""tempo 2 model combine with googlevision api

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lRRVUsTqAlUN5lI65dAvXw1N6-Je0qMp
"""

import shutil
import os

# Clone the GitHub repository
!git clone https://github.com/Sridhar144/files.git

# Move the modelnew.h5 file to /content directory
shutil.move('/content/files/majorprojstage2kaggletitle/modelnew.h5', '/content')

# Optional: Remove the cloned repository if no longer needed

from keras.models import load_model
from collections import deque
import matplotlib.pyplot as plt
import imutils
import numpy as np
import argparse
import pickle
import cv2
from google.colab.patches import cv2_imshow
import os
import time
from keras.models import load_model
from collections import deque


def print_results(video, limit=None):
        #fig=plt.figure(figsize=(16, 30))
        if not os.path.exists('output'):
            os.mkdir('output')

        print("Loading model ...")
        model = load_model('./modelnew.h5')
        Q = deque(maxlen=128)
        vs = cv2.VideoCapture(video)
        writer = None
        (W, H) = (None, None)
        count = 0
        while True:
            # read the next frame from the file
            (grabbed, frame) = vs.read()
            # if the frame was not grabbed, then we have reached the end of the stream
            if not grabbed:
                break

            # if the frame dimensions are empty, grab them
            if W is None or H is None:
                (H, W) = frame.shape[:2]

            # clone the output frame, then convert it from BGR to RGB
            # ordering, resize the frame to a fixed 128x128, and then perform mean subtraction

            output = frame.copy()

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = cv2.resize(frame, (128, 128)).astype("float32")
            frame = frame.reshape(128, 128, 3) / 255

            # make predictions on the frame and then update the predictions
            preds = model.predict(np.expand_dims(frame, axis=0))[0]
            Q.append(preds)

            # perform prediction averaging over the current history of previous predictions
            results = np.array(Q).mean(axis=0)
            i = (preds > 0.50)[0]
            label = i

            text_color = (0, 255, 0) # default : green
            count_violence=0
            count_Nonviolence=0
            if label: # Violence prob
                text_color = (0, 0, 255) # red
                count_violence=count_violence+1
            else:
                text_color = (0, 255, 0)
                count_Nonviolence=count_Nonviolence+1

            text = "Violence: {}".format(label)
            FONT = cv2.FONT_HERSHEY_SIMPLEX
            cv2.putText(output, text, (35, 50), FONT,1.25, text_color, 3)
            # check if the video writer is None
            if writer is None:
                # initialize our video writer
                fourcc = cv2.VideoWriter_fourcc(*"MJPG")
                writer = cv2.VideoWriter("output/v_output.avi", fourcc, 30,(W, H), True)
            # write the output frame to disk
            writer.write(output)

            # show the output image
            cv2_imshow(output)
            key = cv2.waitKey(1) & 0xFF

            # if the `q` key was pressed, break from the loop
            if key == ord("q"):
                break
        # release the file pointersq
        print("[INFO] cleaning up...")
        #writer.release()
        vs.release()
        if count_violence==1:
            print("Violence Detected")
        else:
            print("NO Violence Detected")

V_path = "/content/files/secondmodel/normal1.mp4"
NV_path = "/content/files/firstmodel/Fight_sample_video.mp4"
print_results(V_path)

print_results(NV_path)

